{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assessment 3: WebCrawler and NLP System \n",
    "# Lynne Parkinson"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a44b09f81631678"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TASK 2 - WebCrawler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56ea90f5920a611b"
  },
  {
   "cell_type": "markdown",
   "id": "f7144049",
   "metadata": {},
   "source": [
    "# a. ABC SELENIUM URL List Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365ea734c1cb25b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/get-all-text-of-the-page-using-selenium-in-python/\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "chromeOptions = Options()\n",
    "chrome_prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "chromeOptions.add_experimental_option(\"prefs\", chrome_prefs)\n",
    "chromeOptions.add_argument(\"--disable-infobars\")\n",
    "chromeOptions.add_argument(\"--disable-extensions\")\n",
    "chromeOptions.add_argument(\"--ignore-certificate-errors\")\n",
    "chromeOptions.add_argument(\"--incognito\")\n",
    "chromeOptions.add_argument('--headless')\n",
    "chromeOptions.add_argument(\"--no-sandbox\")\n",
    "chromeOptions.add_argument('disable-notifications')\n",
    "chrome_prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "chromeOptions.add_experimental_option(\"prefs\", chrome_prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=chromeOptions)\n",
    "\n",
    "with open(\"ABC_Links.csv\") as in_file:\n",
    "    for url in in_file:\n",
    "        # get website\n",
    "        driver.get(url.strip())\n",
    "        # get current url\n",
    "        print(driver.current_url)\n",
    "        # Storing the page source in page variable \n",
    "        page = driver.page_source.encode('utf-8') #https://www.geeksforgeeks.org/get-contents-of-entire-page-using-selenium/\n",
    "        file_ = open('result.html', 'ab')\n",
    "        # Write the entire page content in result.html \n",
    "        file_.write(page) \n",
    "        time.sleep(5)\n",
    "  \n",
    "# Closing the file \n",
    "file_.close() \n",
    "print('end of scrape')\n",
    "  \n",
    "# Closing the driver \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b4475",
   "metadata": {},
   "source": [
    "# b. ABC BS4 parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a991f2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:00:24.321927400Z",
     "start_time": "2023-12-09T03:00:22.568928100Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required libraries and packages\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834e22f10478954b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:01:46.593015500Z",
     "start_time": "2023-12-09T03:00:29.363907500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://oxylabs.io/blog/beautiful-soup-parsing-tutorial\n",
    "#finding html tags\n",
    "with open('result.html', 'r', encoding=\"utf8\") as f:\n",
    "    contents = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "\n",
    "    for child in soup.descendants:\n",
    "\n",
    "       if child.name:\n",
    "           print(child.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709414a06c9d372d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:15:50.121981100Z",
     "start_time": "2023-12-09T03:15:49.872648400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all instances of title, extract text and save to txt.file\n",
    "title_list = soup.find_all('h1')\n",
    "with open('title.txt', 'w', encoding='utf-8') as f:\n",
    "    for title in title_list:\n",
    "        f.write(f'{title.text}\\n')\n",
    "        print(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14ce5cf9cfd642f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:15:21.575056200Z",
     "start_time": "2023-12-09T03:15:21.396533200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all instances of p, extract text and save to txt.file\n",
    "body_list = soup.find_all('p')\n",
    "with open('body.txt', 'w', encoding='utf-8') as f:\n",
    "    for body in body_list:\n",
    "        f.write(f'{body.text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b1720933b8e7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-09T03:10:03.906037600Z",
     "start_time": "2023-12-09T03:10:03.747461900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all instances of time, extract text and save to file \n",
    "tim_list = soup.find_all('time')\n",
    "with open('tim.txt', 'w', encoding='utf-8') as f:\n",
    "    for tim in tim_list:\n",
    "        f.write(f'{tim.text}\\n')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Task 3 Data Wrangling and Exploration of Corpus"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b84293c41c81d4b4"
  },
  {
   "cell_type": "markdown",
   "id": "6e4a7a6bb2edf2c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# a. Explore Sample (all documents, not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4288d0c7fc5e64",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Text exploratory analysis\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b22f4c30bb1ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample =  pd.read_fwf('body2.txt', sep=\"\", header=None)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0bc58b3c459fe7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove NANS\n",
    "sample=sample.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee722af4c5302f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save sample to csv\n",
    "sample.to_csv('body.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fceeb50ee55b80",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLOTTING WORD-COUNT\n",
    "sample['word_count'] = sample[0].apply(lambda x: len(str(x).split()))\n",
    "fig=plt.hist(sample['word_count'])\n",
    "plt.hist(sample['word_count'],color='blue', label=None)\n",
    "plt.title('Article Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae902106270380e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prepare body into one giant string\n",
    "big_string = \" \".join(sample[0].values)\n",
    "print(f\"***** Extract of sample big_string ***** \\n{big_string[:101]}\", \"\\n\")\n",
    "# Split sample by white space\n",
    "splits = big_string.split()  \n",
    "print(f\"***** Extract of sample splits ***** \\n{splits[:18]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6160b2a3b583244",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of strings\n",
    "print(f\"Number of strings: {len(splits)}\")\n",
    "print(f\"Number of unique strings: {len(set(splits))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9421c16bba4a30",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most frequent splits\n",
    "freq_splits = FreqDist(splits)\n",
    "print(f\"***** 10 most common strings ***** \\n{freq_splits.most_common(10)}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7e2797d9d1d84",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Short splits\n",
    "short = set(s for s in splits if len(s)<4)\n",
    "short = [(s, freq_splits[s]) for s in short]\n",
    "short.sort(key=lambda x:x[1], reverse=True)\n",
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2470228e4a2c04c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Long splits\n",
    "long = set(s for s in splits if len(s)>10)\n",
    "long = [(s, freq_splits[s]) for s in long]\n",
    "long.sort(key=lambda x:x[1], reverse=True)\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0cb78061642571",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarise patterns and strings\n",
    "def summarise(pattern, strings, freq):\n",
    "    \"\"\"Summarise strings matching a pattern.\"\"\"\n",
    "    # Find matches\n",
    "    compiled_pattern = re.compile(pattern)\n",
    "    matches = [s for s in strings if compiled_pattern.search(s)]\n",
    "    \n",
    "    # Print volume and proportion of matches\n",
    "    print(\"{} strings, that is {:.2%} of total\".format(len(matches), len(matches)/ len(strings)))\n",
    "    \n",
    "    # Create list of tuples containing matches and their frequency\n",
    "    output = [(s, freq[s]) for s in set(matches)]\n",
    "    output.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf03defb5d6c90",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find strings possibly containing html tag\n",
    "summarise(r\"/?>?w*<|/>\", splits, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53f21ee60bb38",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find strings possibly containing numbers\n",
    "summarise(r\"\\d\", splits, freq_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda2f14",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f60f39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# b. Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d78fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') # for sent_tokenize\n",
    "nltk.download('wordnet') # for WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer('english')\n",
    "\n",
    "# Data manipulation/analysis\n",
    "import numpy as np\n",
    "\n",
    "# Text preprocessing/analysis\n",
    "import re\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", context='talk', \n",
    "        palette=['#D44D5C', '#43AA8B'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f03d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open Parsed Data\n",
    "# convert csv file to data frame\n",
    "data_df = pd.read_csv(\"body2.txt\", sep='\\t',\n",
    "                         names=['Body', 'Body2', 'Body3'])\n",
    " # printing data frame\n",
    "print(\"Data frame\")\n",
    "print(data_df)\n",
    " \n",
    "# printing row header\n",
    "print(\"Row header\")\n",
    "print(list(data_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf768e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose 'body' only\n",
    "data=data_df[['Body']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172c2a7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to clean data\n",
    "def clean_data(df, col, clean_col):\n",
    "\n",
    "    # change to lower and remove spaces on either side\n",
    "    df[clean_col] = df[col].apply(lambda x: x.lower().strip())\n",
    "\n",
    "    # remove extra spaces in between\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub(' +', ' ', x))\n",
    "\n",
    "    # remove punctuation\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "\n",
    "    # remove stopwords and get the stem\n",
    "    df[clean_col] = df[clean_col].apply(lambda x: ' '.join(st.stem(text) for text in x.split() if text not in stop_words))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe12749",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calling cleaning function\n",
    "corpus = clean_data(data, 'Body', 'clean_body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a8441",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus=corpus[['clean_body']]\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c06a94",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus.to_csv('clean_body.csv', index=None)\n",
    "corpus.to_csv('clean_body_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ae48a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# c. Explore Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee5dd7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Word Count\n",
    "count = corpus['clean_body'].str.split().str.len()\n",
    "count.index = count.index.astype(str) + ' words:'\n",
    "count.sort_index(inplace=True)\n",
    "count\n",
    "df=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bf7f1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Corpus word length\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.countplot(y=count)\n",
    "plt.title(\"Corpus Text Length Distribution\", size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f5ac2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare corpus into one giant string\n",
    "print(f\"***** Extract of corpus string ***** \\n{corpus[:101]}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ce082",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split corpus by white space\n",
    "corpus = corpus.to_string() \n",
    "splits = corpus.split()  \n",
    "print(f\"***** Extract of corpus splits ***** \\n{splits[:18]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f94acb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of strings\n",
    "print(f\"Number of strings: {len(splits)}\")\n",
    "print(f\"Number of unique strings: {len(set(splits))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36266e78",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most frequent splits\n",
    "freq_splits = FreqDist(splits)\n",
    "print(f\"***** 10 most common strings ***** \\n{freq_splits.most_common(10)}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac91b4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Short splits\n",
    "short = set(s for s in splits if len(s)<4)\n",
    "short = [(s, freq_splits[s]) for s in short]\n",
    "short.sort(key=lambda x:x[1], reverse=True)\n",
    "short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ca994",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Long splits\n",
    "long = set(s for s in splits if len(s)>10)\n",
    "long = [(s, freq_splits[s]) for s in long]\n",
    "long.sort(key=lambda x:x[1], reverse=True)\n",
    "long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a7f47",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarise patterns and strings\n",
    "def summarise(pattern, strings, freq):\n",
    "    \n",
    "        # Find matches\n",
    "    compiled_pattern = re.compile(pattern)\n",
    "    matches = [s for s in strings if compiled_pattern.search(s)]\n",
    "    \n",
    "    # Print volume and proportion of matches\n",
    "    print(\"{} strings, that is {:.2%} of total\".format(len(matches), len(matches)/ len(strings)))\n",
    "    \n",
    "    # Create list of tuples containing matches and their frequency\n",
    "    output = [(s, freq[s]) for s in set(matches)]\n",
    "    output.sort(key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb3c23",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find strings possibly containing html tag\n",
    "summarise(r\"/?>?w*<|/>\", splits, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4acb96",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find strings possibly containing html tag\n",
    "summarise(r\"\\d\", splits, freq_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d3fdfd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What are the most frequent stop words?\n",
    "stop_words = stopwords.words(\"english\")\n",
    "print(f\"There are {len(stop_words)} stopwords.\\n\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584ec7b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extend stopwords\n",
    "stop_words.extend([\"cannot\", \"could\", \"done\", \"let\", \"may\" \"mayn\",  \"might\", \"must\", \"need\", \"ought\", \"oughtn\", \"shall\", \"would\", \"br\"])\n",
    "print(f\"There are {len(stop_words)} stopwords.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daef523",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Frequency of stopwords in corpus\n",
    "freq_stopwords = [(sw, tokens_norm.count(sw)) for sw in stop_words]\n",
    "freq_stopwords.sort(key=lambda x: x[1], reverse=True)\n",
    "freq_stopwords[:192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a741c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the average number of characters per token?\n",
    "# Create list of token lengths for each token\n",
    "token_length = [len(t) for t in tokens]\n",
    "# Average number of characters per token\n",
    "print(f\"Average number of characters per token: {round(np.mean(token_length),4)}\")\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.countplot(y=token_length)\n",
    "plt.title(\"Counts of token length\", size=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d92447c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# d. Prepare corpus for ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119008b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "# tokeniser = RegexpTokenizer(\"[A-Za-z]+\")\n",
    "tokens = tokeniser.tokenize(corpus)\n",
    "with open(\"tokens.txt\", \"w\") as output:\n",
    "    output.write(str(tokens))\n",
    "print(tokens[:20], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcf2c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Number of unique tokens: {len(set(tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02766c7a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Leammatise\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "tokens_norm = [lemmatiser.lemmatize(t.lower(), \"v\") for t in tokens]\n",
    "with open(\"lemmatise.txt\", \"w\") as output:\n",
    "    output.write(str(tokens_norm))\n",
    "print(f\"Number of unique tokens: {len(set(tokens_norm))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39745c21b0335fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 4 -  Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# a. Tfidf and K-means classifier analyses "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba7e0d6f364b408b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "ntlk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare corpus for ML"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9279b51f5715fbb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ce8ec34c77eea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news =pd.read_csv('clean_body.csv')\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d603f3ea8bbca1f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split Corpus into train_test\n",
    "train, test = train_test_split(news['clean_body'], test_size=0.2)\n",
    "print(f\"No. of training examples: {train.shape[0]}\")\n",
    "print(f\"No. of testing examples: {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5ff9c206aee6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenise train data\n",
    "train = train.to_string() \n",
    "tokeniser = RegexpTokenizer(\"[A-Za-z]+\")\n",
    "tokens_train = tokeniser.tokenize(train)\n",
    "print(tokens[:20], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74740dbb3f9457a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lemmatise train data\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "train_norm = [lemmatiser.lemmatize(t.lower(), \"v\") for t in tokens_train]\n",
    "print(f\"Number of unique tokens: {len(set(tokens_norm))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f01a3d52b87371",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenise test data\n",
    "test = test.to_string() \n",
    "tokeniser = RegexpTokenizer(\"[A-Za-z]+\")\n",
    "tokens_test = tokeniser.tokenize(test)\n",
    "print(tokens[:20], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534f34f9d4a89cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Lemmatise test data\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "test_norm = [lemmatiser.lemmatize(t.lower(), \"v\") for t in tokens_test]\n",
    "print(f\"Number of unique tokens: {len(set(tokens_norm))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f4450f951255f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tf-Idf Vectorisation\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(train_norm) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3096a5720540c7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_train_vectors_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6901948f9d3aaab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_test_vectors_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b351e1a45745111",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale the train data\n",
    "scaler = StandardScaler(with_mean= False)\n",
    "scaled_train = scaler.fit_transform(X_train_vectors_tfidf)\n",
    "display(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782628eb4fafa2ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the sparse train matrix  # This was not useful so not included in report\n",
    "# Get the non-zero indices and values of the sparse matrix\n",
    "sparse_matrix=scaled_train\n",
    "nonzero_indices = sparse_matrix.nonzero()\n",
    "nonzero_values = sparse_matrix.data\n",
    "\n",
    "# Create a scatter plot of the sparse matrix\n",
    "plt.scatter(nonzero_indices[1], nonzero_indices[0], c=nonzero_values, cmap='YlGnBu')\n",
    "plt.colorbar()\n",
    "plt.title('Scaled Train Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab33badd7255d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale the test data\n",
    "scaler = StandardScaler(with_mean= False)\n",
    "scaled_test = scaler.fit_transform(X_test_vectors_tfidf)\n",
    "display(scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c0fc3b3e2c56d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the sparse test matrix  # This was not useful so not included in report\n",
    "# Get the non-zero indices and values of the sparse matrix\n",
    "sparse_matrix=scaled_test\n",
    "nonzero_indices = sparse_matrix.nonzero()\n",
    "nonzero_values = sparse_matrix.data\n",
    "\n",
    "# Create a scatter plot of the sparse matrix\n",
    "plt.scatter(nonzero_indices[1], nonzero_indices[0], c=nonzero_values, cmap='YlGnBu')\n",
    "plt.colorbar()\n",
    "plt.title('Scaled Test Scatter Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42304212c7fe0ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Testing different cluster sizes to identify elbow for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb5150ce108e47",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(1, 4))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, n_init=1)\n",
    "    km.fit(scaled_train)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e77d89408189d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(5, 10))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, n_init=1)\n",
    "    km.fit(scaled_train)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75f1c4971c3d69",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(9, 15))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, n_init=1)\n",
    "    km.fit(scaled_train)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4a97c972d3535",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(16, 200))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, n_init=1)\n",
    "    km.fit(scaled_train)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "# Plot sse against k\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(list_k, sse, '-o')\n",
    "plt.xlabel(r'Number of clusters *k*')\n",
    "plt.ylabel('Sum of squared distance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae42973ef3004a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using PCA / SVD to reduce dimensions, before  k-means: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45fdd2cab7d6c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/implementation-of-principal-component-analysis-pca-in-k-means-clustering-b4bc0aa79cb6\n",
    "wcss = []\n",
    "for i in range(1,11):\n",
    "   model = KMeans(n_clusters = i, init = \"k-means++\")\n",
    "   model.fit(scaled_train)\n",
    "   wcss.append(model.inertia_)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,11), wcss)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') # sum of squares of distances of datapoints\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6c0c305311006",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA would not accept sparse matrix\n",
    "pca = PCA(2)  \n",
    "data = pca.fit_transform(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c6c6c45bf8f247",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try SVD to reduce dimensionality - rejected\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "X = csr_matrix(scaled_train)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd.fit(X)\n",
    "\n",
    "print(svd.explained_variance_ratio_)  # paltry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585dd115f1dcf76",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(svd.explained_variance_ratio_.sum()) # paltry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d90f47a908ca7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(svd.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab9a81a4a13977",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot variance explained\n",
    "var_explained = svd.explained_variance_ratio_\n",
    "var_explained\n",
    " \n",
    "sns.barplot(x=list(range(1,len(var_explained)+1)),\n",
    "            y=var_explained, color=\"teal\")\n",
    "plt.xlabel('SVs', fontsize=16)\n",
    "plt.ylabel('Percent Variance Explained', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f7d324068798c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plotting clusters against silhouette score- to determine optimum number of k-means clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4325f575768a57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A list holds the silhouette coefficients for each k\n",
    "# 1 TO 10\n",
    "silhouette_coefficients = []\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_train)\n",
    "    score = silhouette_score(scaled_train, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(1, 11), silhouette_coefficients)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3368974fba6293a3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  9 TO 20\n",
    "silhouette_coefficients = []\n",
    "for k in range(9, 20):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_train)\n",
    "    score = silhouette_score(scaled_train, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(9, 20), silhouette_coefficients)\n",
    "plt.xticks(range(9, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b209a1b515503",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 19 TO 50\n",
    "silhouette_coefficients = []\n",
    "for k in range(19, 50):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_train)\n",
    "    score = silhouette_score(scaled_train, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(19, 50), silhouette_coefficients)\n",
    "plt.xticks(range(19, 50))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a263b4e27fe9833",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# THIS GRAPH WAS CHOSEN -63 CLUSTERS\n",
    "# 49 TO 70\n",
    "silhouette_coefficients = []\n",
    "for k in range(49, 70):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_train)\n",
    "    score = silhouette_score(scaled_train, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(49, 70), silhouette_coefficients)\n",
    "plt.xticks(range(49, 70))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aabc6d5a25c121",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "silhouette_coefficients = []\n",
    "for k in range(69, 90):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_train)\n",
    "    score = silhouette_score(scaled_train, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(69, 90), silhouette_coefficients)\n",
    "plt.xticks(range(69, 90))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b7fbd01318d4f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 89 TO 100\n",
    "silhouette_coefficients = []\n",
    "for k in range(89, 100):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300,random_state=42)\n",
    "    kmeans.fit(scaled_train)\n",
    "    score = silhouette_score(scaled_train, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "    \n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(range(89, 100), silhouette_coefficients)\n",
    "plt.xticks(range(89, 100))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935cef90cab112b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function for K-means fit and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1ad88c408feda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        # labels=km.labels_\n",
    "        train_times.append(time() - t0)\n",
    "        #labels = map(lambda x: dict(enumerate(X['target_names']))[x], X['target'])\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} � {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} � {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339837d83c5f80a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fitting k-means to scaled-train corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802be2512f721c31",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmTR = KMeans(\n",
    "    n_clusters=63,\n",
    "    max_iter=500,\n",
    "    n_init=1,\n",
    ")\n",
    "labels=kmTR.fit_predict(scaled_train)\n",
    "fit_and_evaluate(kmTR, scaled_train, name=\"KMeans\",  n_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b016ee75fca6630",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting clusters for scaled train\n",
    "# https://stackoverflow.com/questions/50898910/visualize-sparse-input-from-sklearn-kmeans-with-matplotlib\n",
    "\n",
    "kmTR = KMeans(n_clusters = 63, init = 'k-means++', max_iter = 500, n_init = 1, verbose = False )\n",
    "kmTR.fit(scaled_train)\n",
    "\n",
    "plt.scatter(scaled_train[:, 0].toarray(), scaled_train[:, 1].toarray())\n",
    "centroids = kmTR.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=500, alpha=0.5)\n",
    "plt.title('Visualisation of clusters (scaled train data)', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecffa4535b7b469",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fitting k-means to scaled-test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe13cdd2bc1b0e5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmTE = KMeans(\n",
    "    n_clusters=63,\n",
    "    max_iter=500,\n",
    "    n_init=1,\n",
    ")\n",
    "labels=kmTE.fit_predict(scaled_test)\n",
    "fit_and_evaluate(kmTE, scaled_test, name=\"KMeans\",  n_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0b719e87e5816",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting clusters for scaled test\n",
    "# https://stackoverflow.com/questions/50898910/visualize-sparse-input-from-sklearn-kmeans-with-matplotlib \n",
    "\n",
    "kmTE = KMeans(n_clusters = 63, init = 'k-means++', max_iter = 500, n_init = 1, verbose = False )\n",
    "kmTE.fit(scaled_test)\n",
    "\n",
    "plt.scatter(scaled_test[:, 0].toarray(), scaled_test[:, 1].toarray())\n",
    "centroids = kmTE.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=500, alpha=0.5)\n",
    "plt.title('Visualisation of clusters (scaled test data)', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a007b3c6393bb2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# b. Sentiment and LDA Analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb57ff8e8e93e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05ff7597410a98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in Data\n",
    "news_df=pd.read_csv('clean_body_2.csv', names=['ID', 'News'])\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81831edafc40d9b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/\n",
    "# Make new data frame with 'id' and 'news' fields\n",
    "df_subset = news_df[['ID', 'News']].copy()\n",
    "#data clean-up\n",
    "#remove all non-aphabet characters\n",
    "df_subset['News'] = df_subset['News'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "#convert to lower-case\n",
    "df_subset['News'] = df_subset['News'].str.casefold()\n",
    "print (df_subset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05d4a371268c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_subset = df_subset.dropna()\n",
    "print (df_subset.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ec36b3d034a4f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_subset.drop([1], axis=0, inplace=True)\n",
    "print (df_subset.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment analysis with nltk-VADER"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c61f9077afa42d4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f6fd326d7a664",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up empty dataframe for staging output\n",
    "df1=pd.DataFrame()\n",
    "df1['ID']=['99999999999']\n",
    "df1['sentiment_type']='NA999NA'\n",
    "df1['sentiment_score']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b524671f296a29",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up for sentiment analysis\n",
    "print('Processing sentiment analysis...')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "t_df = df1 \n",
    "for index, row in df_subset.iterrows():\n",
    "    scores = sid.polarity_scores(row[1])\n",
    "    for key, value in scores.items():\n",
    "        temp = [key,value,row[0]]\n",
    "        df1['ID']=row[0]\n",
    "        df1['sentiment_type']=key\n",
    "        df1['sentiment_score']=value\n",
    "        t_df=t_df._append(df1)\n",
    "#remove dummy row with row_id = 99999999999\n",
    "t_df_cleaned = t_df[t_df.ID != '99999999999']\n",
    "#remove duplicates if any exist\n",
    "t_df_cleaned = t_df_cleaned.drop_duplicates()\n",
    "# only keep rows where sentiment_type = compound\n",
    "t_df_cleaned = t_df[t_df.sentiment_type == 'compound']\n",
    "print(t_df_cleaned.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ec8df22513b95",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge dataframes\n",
    "df_output = pd.merge(news_df, t_df_cleaned, on='ID', how='inner')\n",
    "print(df_output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5257aadc1d7a6b34",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_output.to_csv('sentiment_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210a89bf1434e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_output[[\"sentiment_score\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2822cc82691b5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate mean of sentiment_score by ID\n",
    "dfg = df_output.groupby(['ID'])['sentiment_score'].mean()\n",
    "#create a bar plot\n",
    "dfg.plot(kind='bar', title='Sentiment Score', ylabel='Mean Sentiment Score',  xlabel=None, figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175426e1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LDA sentiment analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9fa78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1680832107195,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "4FcatVW6pu-Y",
    "outputId": "7a6cb45f-740d-4bb2-ad5a-446773f1ec8b"
   },
   "outputs": [],
   "source": [
    "# Load needed packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import re\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be0e16a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_data = pd.read_csv('sentiment_scores.csv')\n",
    "dislay(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee221a7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_2_data=news_data[['News', 'sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbbabd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1680832113119,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "bZ9Phq3ppu-d",
    "outputId": "71577b2a-7c20-4e96-cdaa-24552026fc7b"
   },
   "outputs": [],
   "source": [
    "# Generate positive and negative data files\n",
    "pos_news = news_2_data[news_2_data['sentiment_score']>0.2]\n",
    "neg_news = news_2_data[news_2_data['sentiment_score']<-0.2]\n",
    "pos_news.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bacfb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_news.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbd730",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_news.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257bd5b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Positive texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a45018",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1680832116873,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "n0bpct2Hpu-e",
    "outputId": "eda27090-efe4-4b21-b33a-efd232c10792"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation with re for positive news\n",
    "pos_news['News_Processed'] = pos_news['News'].map(lambda x: re.sub('[,\\.!?]', '', str(x)))\n",
    "# Convert lowercase\n",
    "pos_news['News_Processed'] = pos_news['News'].map(lambda x: x.lower())\n",
    "pos_news['News_Processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57efbbac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "executionInfo": {
     "elapsed": 1915,
     "status": "ok",
     "timestamp": 1680832121544,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "ni9SwKUrpu-f",
    "outputId": "94d1e690-1b23-47f6-bce7-cac16e9a60e6"
   },
   "outputs": [],
   "source": [
    "# Generate Positive Wordcloud \n",
    "from wordcloud import WordCloud\n",
    "long_string = ','.join(list(pos_news['News_Processed'].values))\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47efe27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1680832129553,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "AeLBPCkfpu-g",
    "outputId": "129ebd44-062d-4883-9106-88d66c1b518c"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['nan', 'yes', 'no'])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = pos_news.News_Processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4cbe7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1680832132618,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "o2EFY24Lpu-h",
    "outputId": "ff781d42-4bce-4db1-8cda-4128e0655ebe"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1db47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LDA Trialling number of topics (coherence score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c80ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1178,
     "status": "ok",
     "timestamp": 1680832137083,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "wk77jEzOpu-i",
    "outputId": "88d9e159-f464-46fa-fe4b-243163254adf"
   },
   "outputs": [],
   "source": [
    "# LDA model trialling # number of topics (positive only)\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 20 topics\n",
    "#pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3878f63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating perplexity and coherence score\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62beb04",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try 20 topics and check coherence..\n",
    "num_topics = 20\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa5879",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3e459",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try 50 topics and check coherence..\n",
    "# number of topics\n",
    "num_topics = 50\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 20 topics\n",
    "#pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30847f6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84249e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try 5 topics and check coherence..\n",
    "# number of topics\n",
    "num_topics = 5\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 20 topics\n",
    "#pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d9ccc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1aaa7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Chose 3 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bad508",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final - 3 topics\n",
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 3\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c5c99",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating perplexity and coherence score\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e191414",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the positive topics\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9468535",
   "metadata": {
    "id": "fLj81yI5pu-j"
   },
   "source": [
    "   # Negative Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae43687",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 811,
     "status": "ok",
     "timestamp": 1680832140831,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "1HlEFcoOpu-k",
    "outputId": "53e4729c-3c90-4e9e-f826-6e6b4cfdc5f2"
   },
   "outputs": [],
   "source": [
    "# Remove punctuation with re\n",
    "neg_news['News_Processed'] = neg_news['News'].map(lambda x: re.sub('[,\\.!?]', '', str(x)))\n",
    "# Convert lowercase\n",
    "neg_news['News_Processed'] = neg_news['News_Processed'].map(lambda x: x.lower())\n",
    "neg_news['News_Processed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f12de7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1680832145356,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "WDH5P_1Rpu-l",
    "outputId": "5fab0336-4d83-492a-af6e-68d9f561e69e"
   },
   "outputs": [],
   "source": [
    "# Generate negative Wordcloud\n",
    "long_string2 = ','.join(list(neg_news['News_Processed'].values))\n",
    "long_string2 = long_string2.replace(\"nan\",\"\")\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string2)\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d445e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1680832150179,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "lFAGbxNhpu-n",
    "outputId": "50986b79-051a-472f-8495-b3900b944cba"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "data = neg_news.News_Processed.values.tolist()\n",
    "data_words2 = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words2 = remove_stopwords(data_words2)\n",
    "print(data_words2[:10][9][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd4c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1680832152683,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "raj0ZAMhpu-n",
    "outputId": "cbb2f6d7-f0b6-47ae-b046-f5bf86cbdb07"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words2)\n",
    "# Create Corpus\n",
    "texts = data_words2\n",
    "# Term Document Frequency\n",
    "corpus2 = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus2[:10][9][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbda3d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1680832156215,
     "user": {
      "displayName": "KUAA EC",
      "userId": "00523302372949268980"
     },
     "user_tz": -600
    },
    "id": "yVLws689pu-n",
    "outputId": "416ba10b-fdc3-4301-d279-4b1f81a56f6e"
   },
   "outputs": [],
   "source": [
    "# number of topics = 3\n",
    "num_topics = 3\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus2,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 5 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e662187",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus2))  \n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1425b6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the negative topics\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus2, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A1 - recommendation perspective.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
